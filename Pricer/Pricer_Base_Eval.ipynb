{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "og4q8TbX_wHb"
   },
   "source": [
    "# Product Pricer: Baseline Evaluation with LLaMA 3.1-8B\n",
    "\n",
    "This notebook focuses on evaluating the baseline performance of the **LLaMA 3.1-8B model** for predicting product prices from item descriptions. This serves as a benchmark before we proceed with fine-tuning using LoRA and quantization techniques.\n",
    "\n",
    "The workflow includes dataset loading, model loading and evaluation of predictions.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0oUyuBSYW8e"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets requests torch peft bitsandbytes transformers trl accelerate sentencepiece tiktoken matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pt0kZt-Y6ka"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HF3tOeoq8xa6"
   },
   "source": [
    "Let’s begin by setting up a few constants. First, update the Hugging Face username with your own, if you’ve uploaded the dataset during the Dataset Curation step. Along with that, we’ll also define some color constants that will come in handy later when we visualize our predictions during model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IFQ-JBMY6m9"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "HF_USER = \"samrin\" # Replace with your HuggingFace username\n",
    "DATASET_NAME = f\"{HF_USER}/pricer-data\"\n",
    "MAX_SEQUENCE_LENGTH = 182\n",
    "QUANT_4_BIT = True\n",
    "\n",
    "# Used for writing to output in color\n",
    "\n",
    "GREEN = \"\\033[92m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "RED = \"\\033[91m\"\n",
    "RESET = \"\\033[0m\"\n",
    "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CphhbDBVY6pY"
   },
   "outputs": [],
   "source": [
    "# Log in to HuggingFace\n",
    "\n",
    "hf_token = userdata.get('HUGGINGFACE_TOKEN')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIrIVKWNY6sC"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "train = dataset['train']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAqQW0-_Y6u8"
   },
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIoQVeMs-N8t"
   },
   "source": [
    "For this evaluation, we're leveraging **4-bit quantization** to optimize memory and computation.\n",
    "Additionally, optional code for **8-bit quantization** has been provided, allowing you to experiment with alternative configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0x3HZuzY6yc"
   },
   "outputs": [],
   "source": [
    "# Quantization\n",
    "\n",
    "if QUANT_4_BIT:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "else:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oORCE8okY61F"
   },
   "outputs": [],
   "source": [
    "# Load the Tokenizer and the Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg8f_pAcA1nI"
   },
   "source": [
    "Let's define a **function to extract only the numerical price** from a given text. The model's response might include extra words like \"The price is $99\", but we want to isolate just the price—99 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8dx9-SCY66W"
   },
   "outputs": [],
   "source": [
    "def extract_price(s):\n",
    "    if \"Price is $\" in s:\n",
    "      contents = s.split(\"Price is $\")[1]\n",
    "      contents = contents.replace(',','').replace('$','')\n",
    "      match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", contents)\n",
    "      return float(match.group()) if match else 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYKO_4Y-Y6_K"
   },
   "outputs": [],
   "source": [
    "def model_predict(prompt):\n",
    "    set_seed(42)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    attention_mask = torch.ones(inputs.shape, device=\"cuda\")\n",
    "    outputs = base_model.generate(inputs, max_new_tokens=4, attention_mask=attention_mask, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    return extract_price(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-AVqZAnY7B_"
   },
   "outputs": [],
   "source": [
    "model_predict(test[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZIqkVM9BGuS"
   },
   "source": [
    "## Testing\n",
    "\n",
    "Now, let's define a tester class which evaluates a price prediction function by comparing its outputs against ground truth data. It runs predictions on a dataset, calculates errors (absolute error and squared log error), and color-codes results based on accuracy. It also provides a visual scatter plot of predictions vs actual values and reports overall performance metrics like average error, RMSLE, and hit rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFVWQd0lY7FD"
   },
   "outputs": [],
   "source": [
    "class Tester:\n",
    "\n",
    "    def __init__(self, predictor, data, title=None, size=250):\n",
    "        self.predictor = predictor\n",
    "        self.data = data\n",
    "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
    "        self.size = size\n",
    "        self.guesses = []\n",
    "        self.truths = []\n",
    "        self.errors = []\n",
    "        self.sles = []\n",
    "        self.colors = []\n",
    "\n",
    "    def color_for(self, error, truth):\n",
    "        if error<40 or error/truth < 0.2:\n",
    "            return \"green\"\n",
    "        elif error<80 or error/truth < 0.4:\n",
    "            return \"orange\"\n",
    "        else:\n",
    "            return \"red\"\n",
    "\n",
    "    def run_datapoint(self, i):\n",
    "        datapoint = self.data[i]\n",
    "        guess = self.predictor(datapoint[\"text\"])\n",
    "        truth = datapoint[\"price\"]\n",
    "        error = abs(guess - truth)\n",
    "        log_error = math.log(truth+1) - math.log(guess+1)\n",
    "        sle = log_error ** 2\n",
    "        color = self.color_for(error, truth)\n",
    "        title = datapoint[\"text\"].split(\"\\n\\n\")[1][:20] + \"...\"\n",
    "        self.guesses.append(guess)\n",
    "        self.truths.append(truth)\n",
    "        self.errors.append(error)\n",
    "        self.sles.append(sle)\n",
    "        self.colors.append(color)\n",
    "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
    "\n",
    "    def chart(self, title):\n",
    "        max_error = max(self.errors)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        max_val = max(max(self.truths), max(self.guesses))\n",
    "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)\n",
    "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)\n",
    "        plt.xlabel('Ground Truth')\n",
    "        plt.ylabel('Model Estimate')\n",
    "        plt.xlim(0, max_val)\n",
    "        plt.ylim(0, max_val)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def report(self):\n",
    "        average_error = sum(self.errors) / self.size\n",
    "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
    "        hits = sum(1 for color in self.colors if color==\"green\")\n",
    "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
    "        self.chart(title)\n",
    "\n",
    "    def run(self):\n",
    "        self.error = 0\n",
    "        for i in range(self.size):\n",
    "            self.run_datapoint(i)\n",
    "        self.report()\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls, function, data):\n",
    "        cls(function, data).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GZ9Ysa0aINp"
   },
   "outputs": [],
   "source": [
    "Tester.test(model_predict, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOT-M9xjaIP_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD2lHl8HaIST"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
